{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Papers\n",
    "\n",
    "Predict research paper subject category\n",
    "\n",
    "https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv\n",
    "\n",
    "https://en.wikipedia.org/wiki/ArXiv\n",
    "\n",
    "Tensorflow graph schema was generated using tensorflow_gnn's convert_ogb_dataset.py script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ogb\n",
    "%pip install tensorflow_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "import os\n",
    "import tensorflow_gnn as tfgnn\n",
    "from tensorflow_gnn.experimental import sampler\n",
    "from tensorflow_gnn import runner\n",
    "from tensorflow_gnn.models import mt_albis\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph schema and tensor spec\n",
    "graph_schema = tfgnn.read_schema(\"data/schema.pbtxt\")\n",
    "graph_tensor_spec = tfgnn.create_graph_spec_from_schema_pb(graph_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, index splits, and labels\n",
    "dataset = NodePropPredDataset(name = \"ogbn-arxiv\", root = 'dataset/')\n",
    "dataset.pre_process()\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "graph, labels = dataset[0] # dicts representing node features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into GraphTensor\n",
    "use_mock_data = False\n",
    "if use_mock_data:\n",
    "    # Generate randomized mock data for model debugging on a resource-constrained machine\n",
    "    num_nodes = 1000\n",
    "    graph_tensor = tfgnn.random_graph_tensor(graph_tensor_spec, row_lengths_range=(num_nodes,num_nodes+1))\n",
    "    train_idx = np.arange(0, 700, 1, dtype=int)\n",
    "    valid_idx = np.arange(700, 850, 1, dtype=int)\n",
    "    valid_idx = np.arange(850, 1000, 1, dtype=int)\n",
    "    node_features = graph_tensor.node_sets[\"nodes\"].get_features_dict()\n",
    "    node_features[\"#id\"] = np.arange(num_nodes)\n",
    "    node_features[\"labels\"] = labels[:1000]\n",
    "    graph_tensor = graph_tensor.replace_features(node_sets={\"nodes\": node_features})\n",
    "else:\n",
    "    # Transform data dicts into GraphTensor\n",
    "    # Compose tf.train.Example from graph dict, then read into GraphTensor\n",
    "    # (Dict -> tf.train.Example -> GraphTensor)\n",
    "    node_ids = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=np.arange(len(labels)))\n",
    "    )\n",
    "    num_nodes = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=[len(labels)])\n",
    "    )\n",
    "    node_years = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=graph[\"node_year\"].flatten())\n",
    "    )\n",
    "    node_features = tf.train.Feature(\n",
    "        float_list = tf.train.FloatList(value=graph[\"node_feat\"].flatten())\n",
    "    )\n",
    "    node_labels = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=labels.flatten())\n",
    "    )\n",
    "    edges_size = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=[len(graph[\"edge_index\"][0])])\n",
    "    )\n",
    "    edge_sources = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=graph[\"edge_index\"][0])\n",
    "    )\n",
    "    edge_targets = tf.train.Feature(\n",
    "        int64_list = tf.train.Int64List(value=graph[\"edge_index\"][1])\n",
    "    )\n",
    "    graph_example = tf.train.Example(\n",
    "        features=tf.train.Features(feature={\n",
    "            \"nodes/nodes.#id\": node_ids,\n",
    "            \"nodes/nodes.#size\": num_nodes,\n",
    "            \"nodes/nodes.year\": node_years,\n",
    "            \"nodes/nodes.feat\": node_features,\n",
    "            \"nodes/nodes.labels\": node_labels,\n",
    "            \"edges/edges.#size\": edges_size,\n",
    "            \"edges/edges.#source\": edge_sources,\n",
    "            \"edges/edges.#target\": edge_targets\n",
    "        })\n",
    "    )\n",
    "    # Parse the tf.train.Example graph into graph tensor\n",
    "    graph_tensor = tfgnn.parse_single_example(graph_tensor_spec, graph_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model layers\n",
    "# Configure feature processor layers\n",
    "add_readout = tfgnn.keras.layers.AddReadoutFromFirstNode(\"seed\", node_set_name=\"nodes\")\n",
    "move_label_to_readout = tfgnn.keras.layers.StructuredReadoutIntoFeature(\"seed\", feature_name=\"labels\")\n",
    "feature_processors = [\n",
    "    add_readout,\n",
    "    move_label_to_readout,\n",
    "]\n",
    "\n",
    "\n",
    "# Set model hyperparameters\n",
    "num_graph_updates = 4\n",
    "node_state_dim = 129\n",
    "message_dim = 256\n",
    "edge_dropout_rate = 0\n",
    "state_dropout_rate = 0.2\n",
    "l2_regularization = 1e-5\n",
    "\n",
    "\n",
    "# Define node mapping function\n",
    "def set_initial_node_state(node_set, node_set_name):\n",
    "    return tf.keras.layers.Concatenate()(\n",
    "        [node_set[\"feat\"],\n",
    "         tf.cast(node_set[\"year\"], tf.float32) / 2000]  # Normalize year\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure model generator for core gnn update layers\n",
    "def model_fn(graph_tensor_spec: tfgnn.GraphTensorSpec):\n",
    "    graph = inputs = tf.keras.layers.Input(type_spec=graph_tensor_spec)\n",
    "    graph = tfgnn.keras.layers.MapFeatures(node_sets_fn=set_initial_node_state)(graph)\n",
    "    for i in range(num_graph_updates):\n",
    "        graph = mt_albis.MtAlbisGraphUpdate(\n",
    "            attention_type=\"none\",\n",
    "            units=node_state_dim,\n",
    "            message_dim=message_dim,\n",
    "            receiver_tag=tfgnn.SOURCE,\n",
    "            node_set_names=None if i < num_graph_updates-1 else [\"nodes\"],\n",
    "            simple_conv_reduce_type=\"mean|sum\",\n",
    "            edge_dropout_rate=edge_dropout_rate,\n",
    "            state_dropout_rate=state_dropout_rate,\n",
    "            l2_regularization=l2_regularization,\n",
    "            normalization_type=\"layer\",\n",
    "            next_state_type=\"residual\",\n",
    "        )(graph)\n",
    "    return tf.keras.Model(inputs, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure KerasTrainer\n",
    "# Set training hyperparameters\n",
    "if use_mock_data:\n",
    "    global_batch_size = 50\n",
    "else:\n",
    "    global_batch_size = 5000\n",
    "epochs = 10\n",
    "initial_learning_rate = 0.001\n",
    "steps_per_epoch = len(train_idx) // global_batch_size\n",
    "validation_steps = len(valid_idx) // global_batch_size\n",
    "learning_rate = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate, steps_per_epoch*epochs)\n",
    "optimizer_fn = functools.partial(tf.keras.optimizers.legacy.Adam, learning_rate=learning_rate)\n",
    "task = runner.NodeMulticlassClassification(\n",
    "    num_classes=40,\n",
    "    label_feature_name=\"labels\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = runner.KerasTrainer(\n",
    "    strategy=tf.distribute.get_strategy(),\n",
    "    model_dir=\"gnn_model/\",\n",
    "    callbacks=None,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    restore_best_weights=False,\n",
    "    checkpoint_every_n_steps=\"never\",\n",
    "    summarize_every_n_steps=\"never\",\n",
    "    backup_and_restore=False,\n",
    ")\n",
    "\n",
    "# Initialize model exporter\n",
    "model_exporter = runner.KerasModelExporter(output_names=\"paper_category_logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sampling spec & sampling model\n",
    "sampling_spec = tfgnn.sampler.SamplingSpecBuilder(graph_schema, tfgnn.sampler.SamplingStrategy.RANDOM_UNIFORM).seed(\"nodes\").sample(5, \"edges\").build()\n",
    "\n",
    "def edge_sampler(sampling_op: tfgnn.sampler.SamplingOp):\n",
    "  return sampler.InMemUniformEdgesSampler.from_graph_tensor(\n",
    "      graph_tensor, sampling_op.edge_set_name, sample_size=sampling_op.sample_size, name=sampling_op.op_name\n",
    "  )\n",
    "\n",
    "def node_feature_accessor(node_set_name: tfgnn.NodeSetName):\n",
    "  return sampler.InMemIndexToFeaturesAccessor.from_graph_tensor(\n",
    "      graph_tensor, node_set_name\n",
    "  )\n",
    "\n",
    "sampling_model = sampler.create_sampling_model_from_spec(\n",
    "    graph_schema, sampling_spec, edge_sampler, node_feature_accessor, seed_node_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass DatasetProvider, wrapping the sampling spec & model defined above\n",
    "class SubgraphDatasetProvider(runner.DatasetProvider):\n",
    "  def __init__(self, full_graph_tensor: tfgnn.GraphTensor, sampling_model: tf.keras.Model, sampling_spec: tfgnn.sampler.SamplingSpec, split_idx: np.ndarray):\n",
    "    super().__init__()\n",
    "    self.graph_tensor = full_graph_tensor\n",
    "    self.sampling_model = sampling_model\n",
    "    self.sampling_spec = sampling_spec\n",
    "    self.split_idx = split_idx\n",
    "\n",
    "  def get_dataset(self, context: tf.distribute.InputContext) -> tf.data.Dataset:\n",
    "    # returns sampled tf.data.Dataset\n",
    "    seeds = tf.data.Dataset.from_tensor_slices(self.split_idx)\n",
    "    seeds = seeds.batch(1)\n",
    "    seeds = seeds.map(lambda s: tf.RaggedTensor.from_row_lengths(s, tf.ones_like(s)))\n",
    "    seeds = seeds.map(self.sampling_model)\n",
    "    return seeds.unbatch().prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "train_ds_provider = SubgraphDatasetProvider(graph_tensor, sampling_model, sampling_spec, train_idx)\n",
    "valid_ds_provider = SubgraphDatasetProvider(graph_tensor, sampling_model, sampling_spec, valid_idx)\n",
    "test_ds_provider = SubgraphDatasetProvider(graph_tensor, sampling_model, sampling_spec, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN runner orchestrates model training\n",
    "runner.run(\n",
    "    gtspec=graph_tensor_spec,\n",
    "    train_ds_provider=train_ds_provider,\n",
    "    valid_ds_provider=valid_ds_provider,\n",
    "    global_batch_size=global_batch_size,\n",
    "    epochs=epochs,\n",
    "    feature_processors=feature_processors,\n",
    "    model_fn=model_fn,\n",
    "    task=task,\n",
    "    optimizer_fn=optimizer_fn,\n",
    "    trainer=trainer,\n",
    "    model_exporters=[model_exporter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.6173487233298356}\n"
     ]
    }
   ],
   "source": [
    "# Load model.\n",
    "saved_model = tf.saved_model.load(os.path.join(trainer.model_dir, \"export\"))\n",
    "signature_fn = saved_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "\n",
    "def _clean_example_for_serving(graph_tensor):\n",
    "  graph_tensor = graph_tensor.remove_features(node_sets={\"nodes\": [\"labels\"]})\n",
    "  serialized_example = tfgnn.write_example(graph_tensor)\n",
    "  return serialized_example.SerializeToString()\n",
    "\n",
    "# Convert examples to serialized string format.\n",
    "num_test_cases = len(test_idx)\n",
    "test_ds_provider = SubgraphDatasetProvider(graph_tensor, sampling_model, sampling_spec, test_idx[:num_test_cases])\n",
    "demo_ds = test_ds_provider.get_dataset(tf.distribute.InputContext())\n",
    "serialized_examples = [_clean_example_for_serving(gt) for gt in itertools.islice(demo_ds, num_test_cases)]\n",
    "\n",
    "# Inference on training dataset\n",
    "ds = tf.data.Dataset.from_tensor_slices(serialized_examples)\n",
    "# The name \"examples\" for serialized tf.Example protos is defined by the runner.\n",
    "input_dict = {\"examples\": next(iter(ds.batch(num_test_cases)))}\n",
    "\n",
    "# Outputs are in the form of logits.\n",
    "output_dict = signature_fn(**input_dict)\n",
    "logits = output_dict[\"paper_category_logits\"]  # As configured above.\n",
    "probabilities = tf.math.softmax(logits).numpy()\n",
    "classes = probabilities.argmax(axis=1)\n",
    "  \n",
    "# OGB evaluator\n",
    "evaluator = Evaluator(name = \"ogbn-arxiv\")\n",
    "y_true = np.take(labels, np.asarray(np.split(test_idx[:num_test_cases], num_test_cases)))\n",
    "y_pred = np.asarray(np.split(classes, len(classes)))\n",
    "ogb_evaluator_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "result_dict = evaluator.eval(ogb_evaluator_dict)\n",
    "print(result_dict)  # Current accuracy -> {'acc': 0.6173487233298356}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
